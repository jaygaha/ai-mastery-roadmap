# Introduction to Deep Learning with TensorFlow

Welcome to the Deep Learning module! Here, we move beyond traditional algorithms and explore the power of Neural Networks.

## Module Lessons

### 4.1 From Traditional ML to Deep Learning: The Concept of Neural Networks
Located in: `_04_1_The_Concept_of_Neural_Networks`

- Limitations: Why traditional algorithms struggle with complex data.
- The "Hidden" Secret: How neural networks discover their own features.
- Hierarchies: Moving from simple edges to complex objects.

### 4.2 The Perceptron Model: The "Atom" of AI
Located in: `_04_2_Perception_Model_Explained`

- Anatomy: Inputs, Weights, and the Bias.
- Learning Rule: How a machine improves through "trial and error."
- The XOR Wall: Understanding the limitations of simple models and why we need "Deep" networks.

### 4.3 Activations & Loss Functions: The Engine of Learning
Located in: `_04_3_Activations_Loss_Functions`

- **Activation Functions:** The "Spark" (ReLU, Sigmoid, Softmax) that adds non-linearity.
- **Loss Functions:** The "Scorecard" (MSE, Cross-Entropy) that measures error.

### 4.4 Gradient Descent & Backpropagation: How Networks Learn
Located in: `_04_4_Gradient_Backpropagation_Explained`

- **The Goal:** Minimizing the loss function.
- **Gradient Descent:** Adjusted weights to move "downhill" towards the answer.
- **Backpropagation:** The "Blame Game" â€“ calculating who is responsible for the error.

### 4.5 Setup TensorFlow Keras
Located in: `_04_5_Setup_TensorFlow_Keras`

- **Installation:** Setting up your deep learning environment (Windows, Mac, Linux).
- **Core Concepts:** Understanding the relationship between TensorFlow (Backend) and Keras (Frontend).
- **First Model:** Building and training a "Hello World" neural network.

