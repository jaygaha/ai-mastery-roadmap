import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc

print("=== CHURN PREDICTION MODEL: FINAL PERFORMANCE REPORT ===")
print("Generated by: Lead Data Scientist")
print("--------------------------------------------------------\n")

# 1. Setup Data (Simulated Model Outputs for 40 Test Customers)
# 0 = Happy Customer (No Churn), 1 = Leaving Customer (Churn)
y_true_churn = np.array([0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1])
y_pred_proba_churn = np.array([0.1, 0.05, 0.7, 0.2, 0.85, 0.6, 0.15, 0.9, 0.3, 0.25, 0.75, 0.12, 0.68, 0.92, 0.08, 0.18, 0.55, 0.22, 0.8, 0.03, 0.07, 0.72, 0.1, 0.2, 0.65, 0.09, 0.78, 0.88, 0.14, 0.71, 0.06, 0.28, 0.63, 0.11, 0.81, 0.19, 0.95, 0.04, 0.21, 0.77])

# 2. Decision Threshold
# We decide that if probability > 50%, we label them as "Churn Predictions"
threshold = 0.5
y_pred_churn = (y_pred_proba_churn > threshold).astype(int)

# 3. The Confusion Matrix
print(">> CONFUSION MATRIX")
print("   [True Neg (Happy)   False Pos (Annoyed)]")
print("   [False Neg (Lost)   True Pos (Caught)]")
cm = confusion_matrix(y_true_churn, y_pred_churn)
print(cm)
print("\n")

# 4. Detailed Metrics
print(">> DETAILED CLASSIFICATION REPORT")
print(classification_report(y_true_churn, y_pred_churn, target_names=['No Churn', 'Churn']))
print("-" * 30)

# 5. ROC Curve Visualization
fpr, tpr, thresholds = roc_curve(y_true_churn, y_pred_proba_churn)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'Model Performance (AUC = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Guessing')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate (False Alarms)')
plt.ylabel('True Positive Rate (Recall)')
plt.title('ROC Curve: Churn Prediction Model')
plt.legend(loc="lower right")
plt.grid(True, alpha=0.3)
plt.show()

# 6. Executive Summary
print("\n>> EXECUTIVE SUMMARY FOR STAKEHOLDERS:")
print("1. Accuracy: The model is effectively perfect on this test batch (Accuracy ~1.0).")
print("2. Reliability: It caught 100% of the Churners (Recall = 1.0) without raising any False Alarms (Precision = 1.0).")
print("3. Caution: In the real world, 100% accuracy usually means 'Too Good To Be True'.")
print("   - Did we accidentally train on the test data?")
print("   - Is the dataset too simple?")
print("   Recommendation: Validate on a completely new, unseen dataset before full deployment.")
